{"componentChunkName":"component---src-templates-port-template-js","path":"/KVS/","result":{"data":{"markdownRemark":{"html":"<h1>Objective</h1>\n<p>Create a replicated, fault-tolerant, and causally consistent key-value store</p>\n<p>Should run as a collection of communicating instances (meaning key-value pairs are replicated to all instances)\r\nreplicas communicate state changes whether a key/value gets added or removed among each other\r\nthey should respect the causal order of events when updating their copy of the store and expose a causally consistent view of the store to clients\r\nreplicas do not persist the key-value data if one of the replicas loses connnection or stops working (their local key/value would be lost)\r\nreplicas provide causal metadata to clients in responses and clients send causal metadata to replicas in their requests (i.e. \"causal-metadata\" : X)</p>\n<h1>Features:</h1>\n<h2>Mechanism Description</h2>\n<p>Our key-value store system enforces causal consistency by meticulously tracking causal dependencies. This is achieved through the exchange of causal metadata in requests and responses between clients and replicas. This section details the system's approach to manage causal dependencies.</p>\n<h3>Causal Metadata Handling</h3>\n<p><strong>Initial Interaction</strong>: When a client first interacts with the key-value store, it sends a request with the <code>causal-metadata</code> field set to <code>null</code>. This indicates the beginning of its interaction with the store.</p>\n<p><strong>Metadata Propagation</strong>: Each client request and corresponding response includes a <code>causal-metadata</code> field. Clients don't interpret the value of this field but persistently pass the latest received <code>causal-metadata</code> in their subsequent requests.</p>\n<p><strong>Vector Clocks</strong>: Our system primarily utilizes vector clocks to maintain causal relationships. Each replica and client maintains its own vector clock. The vector clock is a critical component of the <code>causal-metadata</code>.</p>\n<h3>Vector Clock Operations</h3>\n<p><strong>Increment on Updates (Rule One)</strong>: Every time a replica performs an update (PUT or DELETE operations), its entry in the vector clock is incremented. This increment reflects the occurrence of a new event (update) at that replica.</p>\n<p><strong>Merging Clocks (Rule Two)</strong>: Upon receiving a request with <code>causal-metadata</code>, a replica merges its vector clock with the one received. It updates its clock entries to the maximum values between its own clock and the received clock. This process ensures the replica acknowledges the latest known states of other replicas.</p>\n<p><strong>Causality Check</strong>: is_causal(meta_clock): Before processing read (GET) requests, the system checks if the requesting client's causal history (based on its <code>causal-metadata</code>) is consistent with the replica's state. If the replica's state is causally ahead, the request is deferred, ensuring read operations respect causal order.</p>\n<h3>System Design Highlights</h3>\n<p><strong>Replica Management</strong>: check_replica_health():, The system dynamically manages replicas, adding or removing them based on their health status. This flexibility ensures robust performance and fault tolerance.</p>\n<p><strong>Data Synchronization</strong>: sync(replica): Replicas periodically synchronize data and vector clocks with each other, ensuring that all replicas maintain a causally consistent state of the key-value store.</p>\n<p><strong>Broadcast Updates</strong>: Updates from one replica are broadcasted to other replicas to maintain a globally consistent state. This includes both the updated key-value pairs and the updated vector clock. A similar approach is used for checking docker containers that are in the view using add_replica_from_view(replica) and remove_replica_from_view(replica):</p>\n<p><strong>Error Handling</strong>: The system is designed to handle various errors gracefully, ensuring reliability and robustness in operations.</p>\n<h3>Client-Replica Interaction</h3>\n<p><strong>Client Request Format</strong>: <code>{ ... \"causal-metadata\": &#x3C;causal-metadata>, ... }</code>\r\n<strong>Replica Response Format</strong>: Includes updated <code>causal-metadata</code> post operation.</p>\n<p>This design ensures that our key-value store operates in a causally consistent manner, respecting the causal relationships of operations across different replicas and clients.</p>\n<h2>Key-Value Store APIs</h2>\n<h3>View Operations</h3>\n<h3>Put request at /view with JSON body {\"socket-address\":\"<a href=\"IP:PORT\">IP:PORT</a>\"}</h3>\n<p>This endpoint adds a new replica to the view.<br>\nIf the <a href=\"IP:PORT\">IP:PORT</a> is not already part of the view, add it to the view.<br>\nResponse code is 201 (Created).<br>\nResponse body is JSON {\"result\": \"added\"}.<br>\nExample</p>\n<pre><code>$ curl --request PUT --header \"Content-Type: application/json\" --data  \r\n\t'{\"socket-address\":&#x3C;NEW-REPLICA>}' http://&#x3C;EXISTING-REPLICA>/view  \r\n\t{\"result\": \"added\"} \n</code></pre>\n<p>If the <a href=\"IP:PORT\">IP:PORT</a> is already part of the view, then be idempotent and do nothing.<br>\nResponse is 200 (Ok).<br>\nResponse body is JSON {\"result\": \"already present\"}.</p>\n<h3>Get request at /view</h3>\n<p>This endpoint retrieves the view from a replica.<br>\nRetrieve the current view unconditionally.<br>\nResponse code is 200 (Ok).<br>\nResponse body is JSON {\"view\": [\"<a href=\"IP:PORT\">IP:PORT</a>\", \"<a href=\"IP:PORT\">IP:PORT</a>\", ...]}.<br>\nThe response body is a JSON object with a key \"view\" to a list of strings identifying the<br>\nreplicas in the view by IP address and port number. For example, if the view contains the<br>\nthree replicas 10.10.0.2:8090, 10.10.0.3:8090, and 10.10.0.4:8090 then the response body is:   {\"view\": [\"10.10.0.2:8090\", \"10.10.0.3:8090\", \"10.10.0.4:8090\"]}</p>\n<h3>Delete request at /view with JSON body {\"socket-address\":\"<a href=\"IP:PORT\">IP:PORT</a>\"}</h3>\n<p>This endpoint removes an existing replica from the view.<br>\nIf the <a href=\"IP:PORT\">IP:PORT</a> is already part of the view, then remove it.<br>\nResponse is 200 (Ok).<br>\nResponse body is JSON {\"result\": \"deleted\"}.<br>\nIf the <a href=\"IP:PORT\">IP:PORT</a> is not already part of the view, then return an error.<br>\nResponse code is 404 (Not Found).<br>\nResponse body is JSON {\"error\": \"View has no such replica\"}.</p>\n<h3>Key-Value Operations</h3>\n<h3>Put request at /kvs/key with JSON body</h3>\n<p>Request body is JSON {\"value\": value, \"causal-metadata\": V}.<br>\nV is null when the PUT does not depend on prior writes.<br>\nResponse is one of:<br>\n201 (Created) {\"result\": \"created\", \"causal-metadata\": V}<br>\n200 (Ok) {\"result\": \"replaced\", \"causal-metadata\": V}<br>\n400 (Bad Request) {\"error\": \"PUT request does not specify a value\"}<br>\n400 (Bad Request) {\"error\": \"Key is too long\"}<br>\n503 (Service Unavailable) {\"error\": \"Causal dependencies not satisfied; try again later\"}</p>\n<h3>Get request at /kvs/key with JSON body</h3>\n<p>Request body is JSON {\"causal-metadata\": V}.<br>\nThe V is null when the client does not know of prior writes.<br>\nResponse is one of:<br>\n200 (Ok) {\"result\": \"found\", \"value\": \"value\", \"causal-metadata\": V}<br>\nThe V indicates a causal dependency on the PUT of key,value.<br>\n404 (Not Found) {\"error\": \"Key does not exist\"}<br>\n503 (Service Unavailable) {\"error\": \"Causal dependencies not satisfied; try again later\"}</p>\n<h3>Delete request at /kvs/key with JSON body</h3>\n<p>Request body is JSON {\"causal-metadata\": V}.<br>\nResponse is one of:<br>\n200 (Ok) {\"result\": \"deleted\", \"causal-metadata\": V}<br>\nThe V indicates a causal dependency on V and this DELETE.<br>\n404 (Not Found) {\"error\": \"Key does not exist\"}<br>\n503 (Service Unavailable) {\"error\": \"Causal dependencies not satisfied; try again later\"}</p>","frontmatter":{"path":"/KVS","title":"Distributed Key-Value Store"}}},"pageContext":{}},"staticQueryHashes":["3649515864","444233483","63159454"],"slicesMap":{}}